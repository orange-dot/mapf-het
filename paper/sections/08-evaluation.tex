\section{Evaluation}
\label{sec:evaluation}

We evaluate the MAPF-HET system across three dimensions: planning algorithm performance, execution layer scalability, and integrated system behavior.

\subsection{Experimental Setup}

\textbf{Planning benchmarks:} Synthetic workspaces with:
\begin{itemize}
    \item 10-100 agents (mixed 1D/2D/3D)
    \item 500-5000 vertices
    \item Task deadlines: 30-300 seconds
    \item Charging stations: 5-20\% of vertices
\end{itemize}

\textbf{Execution benchmarks:} Hardware-in-the-loop with:
\begin{itemize}
    \item STM32G474 Nucleo boards (up to 64 per segment)
    \item PCAN-USB adapters for CAN-FD monitoring
    \item Custom load generators for stress testing
\end{itemize}

\textbf{Integrated tests:} Simulated depot scenarios with:
\begin{itemize}
    \item 500-2000 power modules
    \item 10-50 robotic agents
    \item 24-hour operational cycles
\end{itemize}

\subsection{Planning Performance}

\begin{table}[htbp]
\centering
\caption{Solver Comparison (50 agents, 1000 vertices)}
\label{tab:planning}
\begin{tabular}{@{}lrrr@{}}
\toprule
Algorithm & Time (s) & Nodes & Makespan \\
\midrule
CBS & 45.2 & 12,840 & 142 \\
CBSH & 18.7 & 5,210 & 142 \\
\mixedcbs{} & 12.3 & 3,180 & 142 \\
\mixedcbs{}+Dim & 8.1 & 1,920 & 142 \\
\bottomrule
\end{tabular}
\end{table}

\mixedcbs{} achieves 3.7$\times$ speedup over baseline CBS through dimensional conflict classification. The ``+Dim'' variant with full dimensional resolution strategies provides 5.6$\times$ speedup while maintaining optimal makespan.

\textbf{Energy constraint impact:}

\begin{table}[htbp]
\centering
\caption{E-CBS Performance vs. Charging Station Density}
\label{tab:ecbs}
\begin{tabular}{@{}lrrr@{}}
\toprule
Stations (\%) & Time (s) & Waypoints Added & Success \\
\midrule
5\% & 24.8 & 18.3 & 94\% \\
10\% & 15.2 & 12.1 & 99\% \\
20\% & 11.4 & 8.7 & 100\% \\
\bottomrule
\end{tabular}
\end{table}

Higher charging station density reduces planning time and increases success rate. At 10\% density, E-CBS achieves 99\% success with modest waypoint overhead.

\subsection{Execution Layer Scalability}

\textbf{Coordination latency:}

\begin{table}[htbp]
\centering
\caption{Field Update Latency by Module Count}
\label{tab:latency}
\begin{tabular}{@{}lrrr@{}}
\toprule
Modules & Mean ($\mu$s) & P99 ($\mu$s) & Jitter ($\mu$s) \\
\midrule
64 & 180 & 420 & 85 \\
256 & 195 & 480 & 110 \\
1024 & 220 & 580 & 145 \\
2048 & 245 & 650 & 180 \\
\bottomrule
\end{tabular}
\end{table}

Latency scales sublinearly with module count due to hierarchical segment architecture. P99 latency remains under 1ms even at 2048 modules.

\textbf{Bandwidth utilization:}

At 64 modules per segment with 50ms field update period:
\begin{itemize}
    \item Heartbeat: 81.9 kbps (1.6\%)
    \item Field updates: 327.7 kbps (6.6\%)
    \item Neighbor exchange ($k=7$): 1.72 Mbps (34.4\%)
    \item Total coordination: 2.13 Mbps (42.6\%)
\end{itemize}

The $k=7$ topological coordination reduces traffic by 9$\times$ compared to full-mesh (which would require 4.03 Mbps, exceeding capacity).

\subsection{Convergence Behavior}

\textbf{Load balancing convergence:}

\begin{table}[htbp]
\centering
\caption{Time to 95\% Load Balance}
\label{tab:convergence}
\begin{tabular}{@{}lrr@{}}
\toprule
Modules & Time (ms) & Cycles \\
\midrule
64 & 180 & 3.6 \\
256 & 290 & 5.8 \\
1024 & 410 & 8.2 \\
2048 & 520 & 10.4 \\
\bottomrule
\end{tabular}
\end{table}

Convergence time scales as $O(\log N)$, consistent with the scale-free correlation theory. At 2048 modules, load balancing completes within 520ms (10 coordination cycles).

\textbf{Partition recovery:}

Simulated network partitions (50/50 split) demonstrate:
\begin{itemize}
    \item Partition detection: 300ms (3 missed heartbeats)
    \item Minority freeze activation: <50ms
    \item Reconciliation after healing: 1.2s (leader election + state sync + load ramp)
\end{itemize}

No split-brain decisions were observed across 1000 partition injection tests.

\subsection{Integrated System Performance}

\textbf{24-hour depot simulation:}

\begin{table}[htbp]
\centering
\caption{Integrated System Metrics (1000 modules, 24h)}
\label{tab:integrated}
\begin{tabular}{@{}lr@{}}
\toprule
Metric & Value \\
\midrule
Charging sessions completed & 2,847 \\
Deadline violations & 3 (0.1\%) \\
Energy constraint violations & 0 \\
Conflict-free path execution & 99.7\% \\
Average module utilization & 78.3\% \\
Peak power delivery & 2.8 MW \\
\bottomrule
\end{tabular}
\end{table}

The integrated system demonstrates high reliability with 0.1\% deadline violation rate and zero energy violations over 24 hours of continuous operation.

\textbf{Deadline slack distribution:}

Across all charging sessions:
\begin{itemize}
    \item Mean slack at completion: 45s
    \item Minimum slack: 2s (critical threshold: 10s)
    \item Sessions with slack < 30s: 8.2\%
\end{itemize}

The slack field mechanism successfully prioritizes tight-deadline sessions.

\subsection{Comparison with Baselines}

\textbf{vs. Centralized scheduling:}
\begin{itemize}
    \item Throughput: 97\% of centralized (gradient overhead)
    \item Fault recovery: 520ms vs. 8s (central restart)
    \item Single point of failure: Eliminated
\end{itemize}

\textbf{vs. Metric-based coordination ($d < 10$m):}
\begin{itemize}
    \item Convergence at 1024 modules: 410ms vs. 2.8s
    \item Information propagation: $O(\log N)$ vs. $O(N)$
    \item Bandwidth: 42\% vs. 78\% utilization
\end{itemize}

The topological $k=7$ approach outperforms metric-based alternatives at scale while matching centralized throughput.
